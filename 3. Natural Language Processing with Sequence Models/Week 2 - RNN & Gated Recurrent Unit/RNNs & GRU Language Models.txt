
Week 2: Recurrent Neural Networks for Language Modeling

In course 2, we learnt how to build a n-gram language model using the chained probability to predict the next word (auto-complete). It is a traditional model with so many limitations.
recall: P(w3|w1,w2) = count(w1,w2,w3) / count(w1,w2) -> compute probability in Trigram model.
Limitations: Large N-grams capture dependencies between distant words and need a lot of space and RAM.

Learn about the limitations of traditional language models and see how RNNs and GRUs use sequential data for text prediction. Then build your own next-word generator using a simple RNN on Shakespeare text data!

Objective:

Recurrent Neural Networks
Gated Recurrent Units

****************Application of RNNs**************
RNNs could be used in a variety of tasks ranging from machine translation to caption generation. There are many ways to implement an RNN model:

One to One: given some scores of a championship, you can predict the winner. 
One to Many: given an image, you can predict what the caption is going to be.
Many to One: given a tweet, you can predict the sentiment of that tweet. 
Many to Many: given an english sentence, you can translate it to its German equivalent


very fine link on RNN:
https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks

