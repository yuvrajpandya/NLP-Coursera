Learning Objectives:

N-grams
Conditional probabilities
Text pre-processing
Language modeling
Perplexity metrics: for Language models evaluation
Out of Vocabulary (OOV) words
K-smoothing
Backoff



what if we can compute the probability of a whole sentence! that's autocomplete magic like an experience when composing a mail.
the simple logic is to train an N-gram model based on as large corpora as possible so that it learns the probabilities of a sequence of words. 

Week 1:
- Process the corpus to N-gram Language Model (training the model)
- Handle Out Of Vocabulary (OOV) problem


******N-grams & their probabilities : finding the probability of a sequence of words e.g.: "I am happy because I am learning"

by using the probability of N-gram, we can compute the probability of a whole sentence!

N-gram: An N-gram is a sequence of words

e.g.: "I am happy because I am learning" --> total words in this corpus = m = 7 words

Unigrams: {I, am, happy, learning, because}

Bigrams: {I am, am happy, happy because, because I, am learning}


Probability of Unigram: e.g. what is the probability of a word appearing "I"?
-----------------------
					
count("I")/m = 2/7  formula: P(w) = c(w)/m ... where m is word, m is size of the corpus

Probability of Bigrams: e.g. what is the probability of a word appearing "am" after "I"?
-----------------------

Using the conditional probability concept:
P("am"|"I") = count("I am") / count("I") = 2/2 = 1

P("happy"|"I") = count("I happy")/count("I") = 0/2 = 0 ---> smoothing is required!

P("learning"|"am") = count("am learning")/count("am") = 1/2 

Generalizing the formula: P(y|x) = c(x y)/c(x)


Probability of Trigram:
-----------------------
P(happy|I am) = count(I am happy)/count(I am) = 1/2

Generalizing: P(w3 | w(1 to 2)) = c(w(1 to 2) w3)/c(w(1 to 2)) --> w(1 to 2) is read as word subscript 1 to superscript 2

c(w(1 to 2) w3) --> can be rewritten as c(w1 w2 w3)


************************
Probability of N-grams:

P(wn | w(1 to N-1)) = c(w(1 to N-1) wn)/c(w(1 to N-1)) --> where wn is nth word , w(1 to N-1) is words 1 to N-1 in the sequence


c(w(1 to N-1) wn) --> can be rewritten as c(w(1 to n)) OR c(w1 w2 w3 ... wn)

***********************


Probability of a sequence:

given a sentence, what is its probability?

P(the teacher drinks tea) = ?

Using conditional probability & chain rule:

P(B|A) = P(A B)/P(A) => rewrite this to get P(A B) = P(A) * P(B|A) ... P(A,B) is a joint probability

chain rule:
P(A,B,C,D) = P(A)*P(B|A)*P(C|A B)*P(D|A B C)

based on this chain rule:

P(the teacher drinks tea) = P(the) * P(teacher|the) * P(drinks|the teacher) * P(tea|the teacher drinks)  ---> awesome! we just computed the probability of this sentence!

Problem: but the training corpus never contains the exact sequence. e.g. "the teacher drinks" sequence might not be in the training corpus as it is long & might not match with 
what the user is typing. i.e. c(the teacher drinks) would likely be 0 in above case!
Solution: Markov assumption! Based on a Markov assumption that only last few words matter to predict the probability of the next word, we can apply
the Markov approximation using Bigram.

P(the teacher drinks tea) = P(the) * P(teacher|the) * P(drinks|teacher) * P(tea|drinks) ---> only the previous word is considered!
						     n
			P(w1 to n)    = II P(wi|wi-1) ...product of all conditional probabilities of current & previous word
						    i=1



******Evaluation Metric of a Language Model (LM) : Perplexity ***********************

Perplexity: lower the perplexity better the language model. 
an evaluation metric to test how well the language model generates the text which sounds natural & not look like a machine generated text. 

Formula: 

	m
PP(W) = || P(wi|wi-1) raised to (-1/m)
	i=1

log perplexity to take care of numeric underflows:

logPP(W) = summation(log(P(wi|wi-1))) * (-1/m)

log to the base 2 is used.

Example:

Given the logarithm of these conditional probabilities:

log(P(Mary|<s>))=-2     
log(P(</s>|cats))=-1
log(P(likes|Mary)) =-10
log(P(cats|likes))=-100

Assuming our test set is W=“<s> Mary likes cats </s>”, what is the model’s perplexity?

logPP(W) = (log(P(Mary|<s>)) + log(P(likes|Mary)) + log(P(cat|likes))) + log(P(</s>|cats)) * -1/m

where m = number of words in the test set including </s> but not <s> = 4
      W = sentences in the test set


(-2 + (-10) + (-100)) + (-1)= (-113)*(-1/4)




**********Out Of Vocabulary (OOV) problem***********

how to tackle words not trained/seen by the Language Model? these words are called OOV words.
These words can be replaced with the token "UNK" (Unknown).

Approach to train model for OOV:
1. Min frequency word: include only those words which appear at least twice (or whatever frequency you decide). 
Words appearing only once will be replaced with UNK thereby minimizing the vocab size to the imp onces

2. |V| : size of the vocab -> set the size of the vocab to say 10000 words. and words having more frequencies should be added to the vocab first.



***********K-smoothing & Backoff******************

Add-k smoothing (similar to the Laplacian smoothing)

formula:

P(wi-1|wi) = count(wi-1,wi) + k			where k is a smoothing parameter & V is the size of the vocab
		     -------------------
			 count(wi-1) + (k*V)     ---> why k*V? : because we are adding k term for every word in the vocab V.

Question:
Corpus: “I am happy I am learning”

In the context of our corpus, what is the estimated probability of word “can” following the word “I” using the bigram model and add-k-smoothing where k=3.

Answer:
since the bigram P(can|I) does not exist in the training corpus, we will apply the add-k-smoothing.
P(can|I) = 3/(2+3*4) 

count(can|I) = 0 & count(I) = 2 & k =3 & V = 4 words in the vocab
hence, (0+3) / (2 + 3*4)


*******Backoff*******

If we are trying to compute probability of a trigram P(wn|wn−2,wn−1) but we have no examples of a particular trigram wn−2,wn−1,wn. 
We can instead estimate its probability by using the bigram probability P(wn|wn−1). Similarly, if we don’t have counts to compute bigram P(wn|wn−1), 
we can look to the unigram P(wn)

corpus:
----------------------------
<s>Lyn drinks chocolate</s>
<s>John drinks tea</s>
<s>Lyn eats chocolate</s>

what is the probability of the statement?: "John drinks chocolate"
since we can't compute the probability of a trigram P(chocolate|John drinks) because the training corpus does not contain this - we have to apply a smoothing technique. 

using backoff, we will traverse back to N-1 gram and use its probability since it exists (i.e. "drinks chocolate" exists):

constant * probability of N-1 gram = 0.4 * P(chocolate|drinks) = 0.4*(1/2) = 0.2 --> experimentally 0.4 constant has shown to give better results on a large corpus.

*******Backoff using Linear Interpolation of N-1, N-2, and so on*******

using the weighted probabilities of the N-grams: 
P(chocolate|John drinks) = 0.7 * P(chocolate|John drinks) + 0.2 * P(chocolate|drinks) + 0.1 * P(chocolate) .... where lamda ƛ = weights (0.7+0.2+0.1) = 1
























