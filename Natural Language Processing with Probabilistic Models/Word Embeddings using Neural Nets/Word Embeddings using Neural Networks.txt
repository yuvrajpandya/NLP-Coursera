Word Embeddings using Neural Networks

Learn about how word embeddings carry the semantic meaning of words, which makes them much more powerful for NLP tasks, 
then build your own Continuous bag-of-words model to create word embeddings from Shakespeare text.

Learning Objectives
-------------------------
Gradient descent
One-hot vectors
Neural networks
Word embeddings
Continuous bag-of-words model (CBOW)
Text pre-processing
Tokenization
Data generators


Popular methods to generate word embeddings:

1. word2vec by Google
	- Continuous bag-of-words (CBOW) 
	- Continuous skip-gram/skip-gram with negative sampling


2. GloVe by Stanford (global vectors)


3. fastText by Facebook: 
Which is based on the skip-gram model and takes into account the structure of words by representing words as an n-gram of characters. 
This enables the model to support previously unseen words, known as Out Of Vocabulary (OOV) words, by inferring their embedding from the sequence of characters they are made of 
and the corresponding sequences that it was initially trained on. For example, it would create similar embeddings for kitty and kitten, even if it had never seen the word kitty before.

Advanced methods based on Deep Neural Networks:

These are tunable pre-trained models available - You can fine tune these models using your own corpus to generate high-quality, domain, specific word embeddings
1. BERT by Google
Bidirectional Encoder Representations from Transformers: You can use a pre-trained BERT model to learn word embeddings on a previously unseen corpus.

2. ELMo may have different word embeddings for the word "stable" depending on the context.

3. GPT-2 by OpenAI
Generative Pre-Trained model



/*******************************Continuous bag-of-words (CBOW) model & its architecture*****************************************

Given the context (or the surrounding words), the CBOW model predicts the center word (or a missing word)
Example
------------
Given: 
corpus: "I am happy because I am learning" --> Vocabulary (V) = I, am, happy, because, learning
C = half-size center word = it is a parameter depecting the number of context/surrounding words in each sides of a center word.
say, C = 2: 
For above corpus, 3 trained context & center words will be generated
#1: context: "I am because I"  | center word: "happy"
#2: context: "am happy I am"   | center word: "because"
#3: context: "happy because am learning" | center word: "I
-------------------------------------------------------------
for the context: "I am because I am" the CBOW model will predict the missing center word = "happy"

***************************The shallow dense neural network architecture of the CBOW model************************

								Input						Hidden							Output
								Layer (x)					Layer (h)						Layer  (y_hat)					x dentotes input which is context words vectors
								 _							 _								 _								y_hat denotes output which is a center word vector
x = context words vector  -->   | |		   W_1				| |			W_2				    | |	
	[.....]						| |------>weights---------->| |-------->weights------------>| |					
								| |		  (ReLU act func)	| |		   (softmax act func)   | |				---> y_hat = center word vector [....]
								| |------> b_1	----------->| |--------> b_2   ------------>| |
								|_|		  biases			|_|			biases			    |_|
										z_1 = W_1*X+b_1					z_2 = W_2*h+b_2		y_hat = softmax(z_2)
								# of neurons: V			hyperparameter					# of neurons: V (there are V possiblities hence it can't be a simple Logistic Regression problem!)
													    N = word embedding size


working: 

- the inputs of the model is a vector of contexts words, call it x, and the output is the vector of the predicted center word, call it Y hat.
- the hidden layer h. the size or dimension of the word embeddings is a hyperparameter of the model, which you select yourself. Let's call the size of the word embedding N, where N can typically be hundred to a thousand
- This is a regular feed-forward network, also called a dense neural network. 
- The three layers are fully connected. I'll call the weights matrix between the input layer and the hidden layer W_1, and the bias vector of the hidden layer is b_1
- W_2 is the weighting matrix between the hidden layer and the output layer, and b_2 is the bias vector of the output layer
- These are the matrices in vectors that the neural network will be learning as you train it
- you will derive the word embeddings from the weight of matrices of this neural network
- For the hidden layer, you will use the rectified linear unit function, or ReLU activation function whose output is sent to the next layer i.e. the output layer
- For the output layer, you will use softmax activation function


Dimensions:
Reminder: two matrices of size i,j & j,k can be multiplied as their # of cols & rows are equal.

where V = size of the vocabulary
	  N = size of the word embedding
	  
				Input layer (x)			Weights (W_1)		Bias (b_1)	  	Hidden layer (h)		Weights(W_2)		Bias(b_2)		Output layer (y_hat)

Dimensions			V*1						N*V				  N*1				N*1						V*N					V*1 		 V*1
Formula																		h = ReLU(z_1 = W_1*x+b_1)									y_hat = softmax(z_2 = W_2*h+b_2)

Note: the size of Input layer & Output layer are same!!!

***Batch processing (parameter for processing several examples to train the model quicker)***

If you are designing a neural network for a CBOW model that will be trained on a corpus of 8000 words, and if you want it to learn 400-dimensional word embedding vectors, 
what should be the size of W1, the weighting matrix between the input layer and hidden layer, 
if it is fed training examples in batches of 16 examples represented by a 8000 row by 16 column matrix?

Ans: 400 rows by 8000 columns.

Batch size = M 

Input X = V*M --> input is V rows representing the vocabulary size & M is the number of examples.

In above question: the size of the weighting matrix does not change. hence it is W_1 = N*V = 400*8000



							************ReLU (Rectified Linear Unit) Activation function****************
										Recall: ReLU(z_1 = W_1*x+b_1) --> pass vector z_1 to ReLU function
										
ReLU is a popular general purpose activation function that only activates the neuron every sum of its weighted inputs is positive and passes through this results

ReLU(X) = max(0,X) --> it is a function of a single real number
									z_1				ReLU(z_1)					Google the graph of ReLU function. Very simple to understand.
									[5.1			[5.1									
									-0.3			 0	---> max(0,X) since X is -0.3 the ReLU returns 0.
									-4.6		  	 0
									0.2]		 	 0.2]





							************Softmax Activation function****************
										Recall: softmax(z_2 = W_2*h+b_2)
										
What does Softmax do? when applied to z_2 (the weighted sum of hidden layer), Softmax outputs a vector of real numbers between 0 to 1 summing up to 1 
					  which can be interpreted as the probabilities of exclusive events. 
					  In the case of the continuous bag of words model, when you apply softmax to z_2, you obtain an output vector y_hat with v rows, 
					  where each row corresponds to a word of the vocabulary of the corpus which is the predicted center word.
										
Given the input vector x below, a trained continuous bag-of-words model outputs the vector Å· below. What is the word predicted by the model?

X = [0.25 0.5     0 		0.25] Y_hat = [0.267 0.005 0.726 0.002]   Ans: the word "therefore" has highest probability of 0.726.
	  am   I   therefore	think


							z_2								y_hat
							[0								[y_hat_1	am
							 1								 y_hat_2	happy
							 2			softmax(z_2)		 y_hat_3	therefore				Formula: y_hat_i = exponential raised to z_i
							 3			----------->			.												 --------------------------------------------------
							 .									.												   summation (exponential raised from z_j to z_v) 
							 .									.												
							 .									.												j = 1 to V
							 V]								 y_hat_V]	zebra							The exponential transforms all our inputs to positive numbers 
														probabilities of being a center word			and the divisor normalizes the vector so that the rols sum up to 1
														
														
							z_2							exp(z_2) 				y_hat = softmax(each element of vector z_2)
							[9							[8103					[0.083
							 8			exponential		 2981					 0.03
							 11			------------>	 59874		----->		 0.612
							 10							 22026					 0.225
							 8.5]						 4915]					 0.05]
													summation: 97899




								***********************Cost Function***************************
								
								Cross-Entropy Loss function (lower the J lower the loss and better the prediction!!!)		Info: log of zero = -Infinity
								
								J = -1 * summation(y_actual_k * log(y_hat_k)) ....k = 1 to V		Note: it is y_actual_k * log of y_hat_k where y_hat_k is a predicted probability 
																										  using softmax function and y_actual_k is either 0 or 1 
												
Case 1: Correct Prediction | y_hat = "happy"	y_actual="happy"
														
								y_hat			y_actual			
								[0.083		   [0   k = 1
								 0.03			0   k = 2				-1 * (y_k_3 * ln(y_hat_3))
								 0.612			1	k = 3			J = -1 * (1 * ln(0.612)) = -1 * (-0.49) = 0.49
								 0.225			0   k = 4
								 0.05]			0]  k = 5
							probabilities		classification classes
							
																									Case 1 	-->	loss/error is less because the prediction is correct (reward!)
																									Case 2 	--> loss/error is higher because the prediction is incorrect (penalized!)
Case 2: Incorrect Prediction | y_hat = "am"		y_actual = "happy"

								y_hat			y_actual
					am			[0.96			[0
					because		 0.01		 	 0
					happy		 0.01		 	 1				J = -1 * (1* ln(0.01)) = -1 * (-4.61) = 4.61   
					I			 0.01		 	 0
					learning	 0.01]		 	 0]


								**********************Cost Function for batches***************************
								
-Forward propogation: Pass the set of examples/batch from Input layer to Output layer through hidden layers


Loss: corresponds to 1 example | Cost: corresponds to a batch (M) of input examples
Cost: simply a mean of losses of all examples in the batch

														  m				 V			  i				i 	---->Note: these notations are superscripts as we are iterating over a batch of examples m
								J_batch = -1/m * summation		summation	 y_actual_j * log(y_hat_j)	
														  i=1			 j=1

Recall: Input context words matrices size = Output predicted center words matrices size = V*M  , where V is the size of vocab & M is size of batch




								************************Backpropogation & Gradient Descent**********************
								
					What is it?: To minimize the error, we have to minimize the cost. To minimize the cost we use 2 techniques to update the weights (W_1 & W_2) & biases (b_1 & b_2)
					
					Backpropogation is an algorithm that calculates the partial derivatives or gradients of the cost with respect to the weights and biases of the neural network
					
					Technique 1: Backpropogation: Calculate the partial derivatives of a cost function J_batch w.r.t weights & biases. Starting from the output layer & working way back through hidden layers.
					Technique 1: Gradient Descent: Adjust the weights & biases by alpha learning rate. A smaller alpha allows for more gradual updates to the weights and biases, 
												   whereas a larger number allows for a faster updates of the weights.
												   













