Parts of Speech

Objectives:
part of speech tagging
Markov Chains



***********************
part of speech tagging: applications: speech recognition, detecting named entity (the Effile Tower, Paris, etc.), co-reference 
***********************

POS tags: Noun, Pronoun, Verb, Adverb, Adjective, Preposition, Conjuction, Participle, 

tag the word of speech per the lexical norms of the English language.

adverb	adverb	verb	noun
why 	not 	learn 	something?

***********************
Markov Chains: 
***********************

visualize a directed graph. each state represents a parts of speech. the transition from a state (a tag) to another state (next word/tag in the sentence) is the probability.

why not learn ... : in this example the word learn is a verb. the probability of the next "event" or a word being a noun in the sentence is higher as usually a verb is followed by a noun.

***markov property: it states that all you need to determine the next state is the current state. it simplifies the model. it doesn't need information from previous states

Used in part of speech tagging & hence can be used for speech recognition.

what's Markov chains? 
They're a type of ***stochastic model*** that describes a sequence of possible events. To get the probability for each event, it needs only the states of the previous events. 
The word stochastic just means random or randomness. So a stochastic model incorporates and models processes does have a random component to them. 
A Markov chain can be depicted as a directed graph.

***The Markov property helps keep the model simple by saying all you need to determine the next state is the current states. It doesn't need information from any of the previous state
Going back to the analogy of whether water is in solid, liquid, or gas states. If you look at a cup of water that is sitting outside, the current states of the water is a liquid state. 
When modeling the probability that the water in the cup will transition into the gas states, you don't need to know the previous history of the water. 
Whether it's previously came from ice cubes or whether it's previously came from rain clouds


important words:
transition probabilities
transition matrix (of size n+1,n where n=parts of speech) showing the transition probabilities between each possible pair of states (the states are parts of speech in this case)

why n+1?: we need initial probability to begin with. ùõë : sentence starting token. sentence starting with POS tags.



			NN(Noun)	(VB)Verb	Others(all other parts of speech) ---> current state/word
ùõë 			0.6			0			0.4
Noun		0.2			0.2			0.6									sum(transition probabilities)=1
Verb		0.4			0.3			0.3									sum(transition probabilities)=1
O			0.2			0.3			0.5									sum(transition probabilities)=1



********************************************************************************************************************************************
Hidden Markov Chains model: why "hidden"? from machines perspective the states i.e. parts of speech are not known. e.g. machine does not know that the word "learn" is a verb.
Most commonly used algorithm in NLP & foundation behind many techniques used in deep learning.
In addition to parts-of-speech tagging, HMM is used in speech recognition, speech synthesis, etc. 


A hidden Markov model (HMM) is a statistical model that can be used to describe the evolution of observable events that depend on internal factors, which are not directly observable. 
We call the observed event a `symbol' and the invisible factor underlying the observation a `state'. 
An HMM consists of two stochastic processes, namely, an invisible process of hidden states and a visible process of observable symbols. 
The hidden states form a Markov chain, and the probability distribution of the observed symbol depends on the underlying state. 
For this reason, an HMM is also called a DOUBLY-EMBEDDED STOCHASTIC PROCESS.
********************************************************************************************************************************************
Objective: to find most probable next POS tag or most probable the next word in sequence.

This model has:
1. a transition matrix showing transition probabilities between the parts of speech.

2. Emission Matrix: a n,V matrix : n=parts of speech (states) & V = words in the corpus
The emission matrix represents the probabilities for the transition of your n hidden states representing your parts of speech tags to the V words in your corpus

			going		to		eat		laugh		happy	.... n words in the entire corpus
Noun		0.2			0.2		0.4		0.1			0.1		....   sum(transition probabilities of this sentence)=1.0
Verb
O


********************
Viterbi algorithm: What if you're given a sentence, like "why not learn something?" 
				   What is the most likely sequence of parts of speech tags given the sentence in your model? 
				   The sequence can be computed using the Viterbi algorithm.
********************

The Viterbi is a graph algorithm which would utilize the Hidden Markov Model (transition & emission matrices) to find the sequence of POS tags/or hidden states.

The algorithm can be split into three main steps: 
1. the initialization step
2. the forward pass
3. the backward pass



